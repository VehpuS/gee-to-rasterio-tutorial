[
  {
    "objectID": "gee-to-rasterio.html",
    "href": "gee-to-rasterio.html",
    "title": "Google Earth Engine to Rasterio",
    "section": "",
    "text": "I recommend running in a virtual environment / colab.\n\npip install jupyter_contrib_nbextensions\npip install RISE\npip install geemap\npip install eemont\npip install autopep8\npip install rasterio\npip install numpy\npip install blockdiagMagic\n\n\nimport time\n\nfrom IPython.display import IFrame\n\n\n\n\nRerun jupyter notebook (via the web UI) after installing RISE, then you should see the following button, which opens the slideshow: \n\n\n\n\n\n\nHow to generate a slideshow from a notebook\nAdding Rise to be able to run slideshows from Jupyter\n\n\n\n\nDiagrams should appear when Jupyter is run after the blockdiagMagic is installed.\n\n\nGithub repo\n\n\n\n\n#Reset to png output with: %setdiagpng\n\n\nA -> B -> C;\nB -> D;"
  },
  {
    "objectID": "gee-to-rasterio.html#google-earth-engine",
    "href": "gee-to-rasterio.html#google-earth-engine",
    "title": "Google Earth Engine to Rasterio",
    "section": "Google Earth Engine",
    "text": "Google Earth Engine\nGoogle earth engine combines a multi-petabyte catalog of satellite imagery and geospatial datasets with planetary-scale analysis capabilities\n\nimport ee, eemont, geemap\n# Authenticate and Initialize Earth Engine and geemap.\nee.Authenticate()\nee.Initialize()\n\nTo authorize access needed by Earth Engine, open the following\n        URL in a web browser and follow the instructions:\n        https://code.earthengine.google.com/client-auth?scopes=https%3A//www.googleapis.com/auth/earthengine%20https%3A//www.googleapis.com/auth/devstorage.full_control&request_id=VzhI2ZImWtee-_WBwKLsGe_uGSvjmYKaVjsxHnAtodg&tc=mRtZEsxI7I_YDk82UJ_9Oh-Llg42nZhcKRQslmrj91k&cc=HsMmRv6vzluxUaco-aqESQDftZYlV7k2kejDMMKUoGE\n        The authorization workflow will generate a code, which you\n        should paste in the box below\n        \n\n\nEnter verification code: 4/1AdQt8qhV8GB5g0cU6OgSj888f1-imZujyJEE_MHP8Hgi7k0knrCbjDJCrhk\n\nSuccessfully saved authorization token.\n\n\n\n# Example point of interest to filter the image collection.\npoint = ee.Geometry.Point([-75.92, 2.86])\n# Get and filter the Sentinel-2 Surface Reflectance image collection and filter it by region and time.\nS2 = (ee.ImageCollection('COPERNICUS/S2_SR')\n      .filterDate('2020-01-01','2021-01-01').filterBounds(point))\n# Cloud Probability masking is the default method to mask clouds and shadows in Sentinel-2 (method = 'cloud_prob'). By default, the Cloud Probability threshold is 60% (prob = 60).\nS2c = S2.maskClouds()\n\n\n# Visualization\n# Set the visualization parameters.\nrgbUnscaled = {'min':0, 'max':3000, 'bands':['B4','B3','B2']}\n# Use geemap to display results.\nMap = geemap.Map()\nMap.centerObject(point,15)\nMap.addLayer(S2c.median(),rgbUnscaled,'60% Cloud Probability masking')\nMap\n\n\n\n\nIn case of bad internet / viewing in a repo - here is what the map looks like"
  },
  {
    "objectID": "gee-to-rasterio.html#rasterio",
    "href": "gee-to-rasterio.html#rasterio",
    "title": "Google Earth Engine to Rasterio",
    "section": "Rasterio",
    "text": "Rasterio\nRasterio reads and writes geospatial raster data.\nGeographic information systems use GeoTIFF and other formats to organize and store gridded, or raster, datasets. Rasterio reads and writes these formats and provides a Python API based on N-D arrays.\n\nimport rasterio\nfrom rasterio.plot import show\n\nwith rasterio.open(\"https://oin-hotosm.s3.amazonaws.com/56f9b5a963ebf4bc00074e70/0/56f9c2d42b67227a79b4faec.tif\") as src:\n    show(src)\n\n\n\n\nIn case of bad internet / viewing in a repo, here is what the output looks like:"
  },
  {
    "objectID": "gee-to-rasterio.html#high-level---remote-sensing-pipeline",
    "href": "gee-to-rasterio.html#high-level---remote-sensing-pipeline",
    "title": "Google Earth Engine to Rasterio",
    "section": "High Level - Remote Sensing Pipeline",
    "text": "High Level - Remote Sensing Pipeline\nI’ll be giving the hight level data flow with small demonstrate in code - with the goal of providing an easy starting point to fill the “pipeline” with steps relevant to individual projects.\n\n#Reset to png output with: %setdiagpng\n# Hide the input for the diagram after coding\n\n\n\"Get input data\" -> \"Preprocess data\" -> \"\"\"Google Earth Engine\nProcessing\"\"\" -> \"Exporting data\" -> \"Reading Cloud Images\" -> \"Rasterio Raster Analysis\" -> \"Further Computation\";\n\"\"\"Google Earth Engine\nProcessing\"\"\" -> \"\"\"Display on Google Earth Engine\"\"\";\n\"Reading Cloud Images\" -> \"Visualize Rasters\";\n\n\n\n\n\n\"Get input data\" -> \"Preprocess data\" -> \"\"\"Google Earth Engine\nProcessing\"\"\" -> \"...\"\n\"\"\"Google Earth Engine\nProcessing\"\"\" -> \"\"\"Display on Google Earth Engine\"\"\";\n\n\n\n\n\n\"Exporting data\" -> \"Reading Cloud Images\" -> \"Rasterio Raster Analysis\" -> \"Further Computation\";\n\"Reading Cloud Images\" -> \"Visualize Rasters\";\n\n\n\n\n\n1. Get input data\nGoogle Earth Engine data catalog: https://developers.google.com/earth-engine/datasets\nNote: Some data can be accessed using other providers / using other services, some is unique\n\nSatellite Data\n\nSentinel (1, 2, 3) - 2 is used for current 10m res RGB, 1 for 10m res infrared\nLandsat (4, 5, 7. 8. 9) - 8 is widely used for historical data at 30m res\nHigh-Resolution Imagery\nOthers satellites: MODIS, ALOS\n\n\n\n\n\n\nPrecomputed data from previous analysis (manual or ML based)\n\nSurface Temperature\nClimate\nAtmospheric\nWeather\nElevation Data\nLand Use / Land Cover (LULC)\nMore…\n\n\n \n\n\n\nArea of interest (shape)\n\ngeopolitical border (country / state / city)\ncrop boundaries (precomputed or derived)\nforest boundaries (precomputed or derived)\n\n\n\nUploaded Assets (images, shapes, tabular data)\n\nvia the Google Earth Engine web interface.\n\n\n\n\n\n\n\n2. Preprocess input data\n\nFilter images by geographical bounds and time range\nClip images by shape (different than filtering).\nCloud removal\nCalculate spectral indices (research based measures for various ecological phenomena)\n\nTechnically part of processing, but many spectral indices are so common they are often used as input features for additional analysis\nThere are ready made libraries to calculate them: https://github.com/awesome-spectral-indices/spectral\n\nCalculate derived shapes if relevant\n\n\nJavascript Libraries\nI focus on Python APIs in this presentation as they allow us to work with one programming language accross the pipeline, though Google Earth Engine has a language agnostic (REST) API, and an online coding environment using JavaScript (albeit with a slightly limited syntax) as it’s coding language, accessible via code.earthengine.google.com.\nIf you come accross useful code written in JavaScript, conversion isn’t too hard if you know both languages’ syntax (main challenges are anonymous functions and dictionary parameters changing into keyword parameters in Python). Google provides some support for converting code from JavaScript to Python, demonstrated in this notebook.\n\n\nExample: S2 Preprocessing\nBased on this tutorial\n\nAOI = ee.Geometry.Point([-75.92, 2.86]).buffer(100)\nSTART_DATE = '2020-01-01'\nEND_DATE = '2021-01-01'\nCLOUD_FILTER = 60\nCLD_PRB_THRESH = 50\nNIR_DRK_THRESH = 0.15\nCLD_PRJ_DIST = 1\nBUFFER = 50\n\n\ndef add_cloud_bands(img):\n    # Get s2cloudless image, subset the probability band.\n    cld_prb = ee.Image(img.get('s2cloudless')).select('probability')\n\n    # Condition s2cloudless by the probability threshold value.\n    is_cloud = cld_prb.gt(CLD_PRB_THRESH).rename('clouds')\n\n    # Add the cloud probability layer and cloud mask as image bands.\n    return img.addBands(ee.Image([cld_prb, is_cloud]))\n\n\ndef add_shadow_bands(img):\n    # Identify water pixels from the SCL band.\n    not_water = img.select('SCL').neq(6)\n\n    # Identify dark NIR pixels that are not water (potential cloud shadow pixels).\n    SR_BAND_SCALE = 1e4\n    dark_pixels = img.select('B8').lt(\n        NIR_DRK_THRESH*SR_BAND_SCALE).multiply(not_water).rename('dark_pixels')\n\n    # Determine the direction to project cloud shadow from clouds (assumes UTM projection).\n    shadow_azimuth = ee.Number(90).subtract(\n        ee.Number(img.get('MEAN_SOLAR_AZIMUTH_ANGLE')))\n\n    # Project shadows from clouds for the distance specified by the CLD_PRJ_DIST input.\n    cld_proj = (img.select('clouds').directionalDistanceTransform(shadow_azimuth, CLD_PRJ_DIST*10)\n                .reproject(**{'crs': img.select(0).projection(), 'scale': 100})\n                .select('distance')\n                .mask()\n                .rename('cloud_transform'))\n\n    # Identify the intersection of dark pixels with cloud shadow projection.\n    shadows = cld_proj.multiply(dark_pixels).rename('shadows')\n\n    # Add dark pixels, cloud projection, and identified shadows as image bands.\n    return img.addBands(ee.Image([dark_pixels, cld_proj, shadows]))\n\n\ndef get_s2_sr_cld_col(aoi, start_date, end_date):\n    # Import and filter S2 SR.\n    s2_sr_col = (ee.ImageCollection('COPERNICUS/S2_SR')\n                 .filterBounds(aoi)\n                 .filterDate(start_date, end_date)\n                 .filter(ee.Filter.lte('CLOUDY_PIXEL_PERCENTAGE', CLOUD_FILTER)))\n\n    # Import and filter s2cloudless.\n    s2_cloudless_col = (ee.ImageCollection('COPERNICUS/S2_CLOUD_PROBABILITY')\n                        .filterBounds(aoi)\n                        .filterDate(start_date, end_date))\n\n    # Join the filtered s2cloudless collection to the SR collection by the 'system:index' property.\n    return ee.ImageCollection(ee.Join.saveFirst('s2cloudless').apply(**{\n        'primary': s2_sr_col,\n        'secondary': s2_cloudless_col,\n        'condition': ee.Filter.equals(**{\n            'leftField': 'system:index',\n            'rightField': 'system:index'\n        })\n    }))\n\n\ndef add_cld_shdw_mask(img):\n    # Add cloud component bands.\n    img_cloud = add_cloud_bands(img)\n\n    # Add cloud shadow component bands.\n    img_cloud_shadow = add_shadow_bands(img_cloud)\n\n    # Combine cloud and shadow mask, set cloud and shadow as value 1, else 0.\n    is_cld_shdw = img_cloud_shadow.select('clouds').add(\n        img_cloud_shadow.select('shadows')).gt(0)\n\n    # Remove small cloud-shadow patches and dilate remaining pixels by BUFFER input.\n    # 20 m scale is for speed, and assumes clouds don't require 10 m precision.\n    is_cld_shdw = (is_cld_shdw.focalMin(2).focalMax(BUFFER*2/20)\n                   .reproject(**{'crs': img.select([0]).projection(), 'scale': 20})\n                   .rename('cloudmask'))\n\n    # Add the final cloud-shadow mask to the image.\n    return img_cloud_shadow.addBands(is_cld_shdw)\n\n\ndef display_cloud_layers(col, Map):\n    # Mosaic the image collection.\n    img = col.mosaic()\n\n    # Subset layers and prepare them for display.\n    clouds = img.select('clouds').selfMask()\n    shadows = img.select('shadows').selfMask()\n    dark_pixels = img.select('dark_pixels').selfMask()\n    probability = img.select('probability')\n    cloudmask = img.select('cloudmask').selfMask()\n    cloud_transform = img.select('cloud_transform')\n\n    # Create a map object.\n    Map.centerObject(AOI, 12)\n\n    # Add layers to the Google Earth Engine map.\n    Map.addLayer(img,\n                 {'bands': ['B4', 'B3', 'B2'], 'min': 0,\n                     'max': 2500, 'gamma': 1.1},\n                 'S2 image', True, 1)\n    Map.addLayer(probability,\n                 {'min': 0, 'max': 100},\n                 'probability (cloud)', True, 1)\n    Map.addLayer(clouds,\n                 {'palette': ['e056fd']},\n                 'clouds', True, 1)\n    Map.addLayer(cloud_transform,\n                 {'min': 0, 'max': 1, 'palette': ['white', 'black']},\n                 'cloud_transform', False, 1)\n    Map.addLayer(dark_pixels,\n                 {'palette': ['orange']},\n                 'dark_pixels', True, 1)\n    Map.addLayer(shadows, {'palette': ['yellow']},\n                 'shadows', True, 1)\n    Map.addLayer(cloudmask, {'palette': ['orange']},\n                 'cloudmask', True, 0.5)\n\n\ns2_sr_cld_col_eval = get_s2_sr_cld_col(AOI, START_DATE, END_DATE)\n\ns2_sr_cld_col_eval_disp = s2_sr_cld_col_eval.map(add_cld_shdw_mask)\n\n\nMap = geemap.Map()\ndisplay_cloud_layers(s2_sr_cld_col_eval_disp, Map)\nMap  # Display the map.\n\n\n\n\nIn case of bad internet / viewing in a repo - here is what the map looks like \nNote: most preprocessing / spectral indices calculations can be easily performed using the eemont extension (as seen in the first GEE Example)\nRead more about these features in the eemont library’s documentation\n\nIFrame('https://eemont.readthedocs.io/en/latest/#clouds-and-shadows-masking', width=900, height=500)\n\n\n        \n        \n\n\nIn case of bad internet / viewing in a repo - here is what the documentation looks like \n\n\n\n3. Processing on Google Cloud\nUse Google Cloud via Google Earth Engine to run algorithms directly on large inputs (to save on data transfers and taking advantage of Google’s Cloud).\n\nProvided Statistical / Machine learning models\nTimeseries analysis (i.e. CCDC)\nSampling the data (i.e. by pixel)\nReduction operations (i.e. to value)\nPixel based aggregations\nCustom code running on a Google Cloud Platform (i.e. custom made Tensorflow models).\n\nNote - these operations are performed asynchronously on automatically assigned Google Cloud platforms. Their speed will depend on general usage and the size of the task. Some tasks might fail, requiring reducing the area of interest / amount of images analysed to complete\n\nExample: Supervised Classification\nBased on this tutorial\n\nLANDSAT_8 = ee.ImageCollection('LANDSAT/LC08/C02/T1_L2')\n\nBIT_31 = '11111'\nDEC_31 = int(BIT_31, 2)\n\n# Define a function that scales and masks Landsat 8 surface reflectance images.\n\n\ndef prepSrL8(image):\n    # Develop masks for unwanted pixels (fill, cloud, cloud shadow).\n    # https://www.usgs.gov/media/images/landsat-collection-2-pixel-quality-assessment-bit-index\n    # image.select - Returns an image with the selected bands\n    qaMask = image.select('QA_PIXEL').bitwiseAnd(DEC_31).eq(0)\n    # https://www.usgs.gov/landsat-missions/landsat-collection-2-quality-assessment-bands\n    saturationMask = image.select('QA_RADSAT').eq(0)\n\n    img_dict = image.toDictionary()\n\n    # Apply the scaling factors to the appropriate bands.\n    def getFactorImg(factorNames):\n        factorList = img_dict.select(factorNames).values()\n        return ee.Image.constant(factorList)\n\n    # select accepts regular exceptions\n    scaleImg = getFactorImg(\n        ['REFLECTANCE_MULT_BAND_.|TEMPERATURE_MULT_BAND_ST_B10'])\n    offsetImg = getFactorImg(\n        ['REFLECTANCE_ADD_BAND_.|TEMPERATURE_ADD_BAND_ST_B10'])\n    scaled = image.select('SR_B.|ST_B10').multiply(scaleImg).add(offsetImg)\n\n    # Replace original bands with scaled bands and apply masks.\n    # Image.addBands(srcImg, names, overwrite)\n    return image \\\n        .addBands(scaled, None, True) \\\n        .updateMask(qaMask) \\\n        .updateMask(saturationMask)\n\n\nfirst_start_date = '2021-03-01'\nfirst_end_date = '2021-07-01'\n\nfirst_date_images = LANDSAT_8.filterDate(first_start_date, first_end_date)\n\n# Make a cloud-free Landsat 8 surface reflectance composite.\nfirst_image = first_date_images.map(prepSrL8).median()\n\n# Use these bands for prediction.\nrgb_bands = ['SR_B4',  'SR_B3',  'SR_B2']\nnon_visual_wavelengths = ['SR_B5', 'SR_B6', 'SR_B7']\ntemp_bands = ['ST_B10']\nwavelengths_with_temp_bands = rgb_bands + non_visual_wavelengths + temp_bands\n# SR_B2     Band 2 (blue) surface reflectance\n# SR_B3     Band 3 (green) surface reflectance\n# SR_B4     Band 4 (red) surface reflectance\n# SR_B5     Band 5 (near infrared) surface reflectance\n# SR_B6     Band 6 (shortwave infrared 1) surface reflectance\n# SR_B7     Band 7 (shortwave infrared 2) surface reflectance\n# ST_B10    Band 10 surface temperature. If 'PROCESSING_LEVEL' is set to 'L2SR', this band is fully masked out.\n\nfirst_prediction_bands = first_image.select(wavelengths_with_temp_bands)\n\n# Load training points. The numeric property 'class' stores known labels.\npoints = ee.FeatureCollection('GOOGLE/EE/DEMOS/demo_landcover_labels')\n\n# This property stores the land cover labels as consecutive integers starting from zero.\nlandcover_label = 'landcover'\n\n# Overlay the points on the imagery to get training.\nfirst_training = first_prediction_bands.sampleRegions(**{\n    'collection': points,\n    'properties': [landcover_label],\n    'scale': 30\n})\n\n# Train a CART classifier with default parameters.\nfirst_trained = ee.Classifier.smileCart().train(\n    first_training, landcover_label, wavelengths_with_temp_bands)\n\n# Classify the image with the same bands used for training.\nfirst_classified = first_image.select(\n    wavelengths_with_temp_bands).classify(first_trained)\n\nClassificationMap = geemap.Map()\n\n# Display the inputs and the results.\nClassificationMap.setCenter(-122.0877, 37.7880, 11)\nClassificationMap.addLayer(first_image,\n                           {'bands': rgb_bands, 'min': 0, 'max': 0.25},\n                           'sat-image')\nClassificationMap.addLayer(first_classified,\n                           {'min': 0, 'max': 2, 'palette': [\n                               'orange', 'green', 'blue']},\n                           'classification')\n\n\nClassificationMap\n\n\n\n\nIn case of bad internet / viewing in a repo - here is what the map looks like \n\n\n\nEnding A: Display data on Google Earth Engine\nIn some cases, keeping our code and data on Google Earth Engine is enough, as the platform provides us with the ability to: - generate interactive maps with embedded layers and data - displayed in notebooks or using the JavaScript API - code apps running wholely on the Google Earth Engine plaform - only using the JavaScript API (i.e. CCDC Tools, Dynamic World)\n\nIFrame('https://parevalo_bu.users.earthengine.app/view/visualize-ccdc', width=900, height=250)\n\n\n        \n        \n\n\n\nIFrame('https://dynamicworld.app/explore', width=900, height=350)\n\n\n        \n        \n\n\nIn case of bad internet / viewing in a repo - here is what the apps looks like \n\nAdvantages:\n\nLess code to handle\nGoogle provided basemap\nNo additional costs for hosting / map access\n\n\n\nDisadvantages:\n\nVendor lock-in\nLimited access to data outside the platform (mainly by uploading to GCS / Google Earth Engine directly)\nRequires using Google Cloud and learning Google Earth Engine APIs to run custom code\nUI is limited to Google Earth Engine APIs\nLicensing limitations due to code running on Google Platform\n\n\n\n\n4. Exporting data\nGoogle Earth Engine provides library support to export the following: - Tables - data in columns and rows, i.e. CSV data - Images - 3D arrays of data (multiband images) with various data types and additional metadata - i.e. GeoTiff data - Maps - a rectangular pyramid of map tiles for use with web map viewers (images at different zoom levels).\nData can be exported to - Google Earth Engine as assets - Google Drive as files - Google Cloud Storage as files on “bucket” storage (storage with no predefined size limit and apis used for ease in streaming the data). * Note: GeoTIFS in Google Cloud Storage can be read directly using the ee.Image.loadGeoTIFF static method (docs).\n\nExample: Code to Handle Exports\nBased on this tutorial\n\none_hour_in_secs = 60 * 60\n\n\ndef task_status(task_id):\n    \"\"\"Fetches the current status of the task.\n\n    Returns:\n      A dictionary describing the current status of the task as it appears on\n      the EE server. Includes the following fields:\n      - state: One of the values in Task.State.\n      - creation_timestamp_ms: The Unix timestamp of when the task was created.\n      - update_timestamp_ms: The Unix timestamp of when the task last changed.\n      - output_url: URL of the output. Appears only if state is COMPLETED.\n      - error_message: Failure reason. Appears only if state is FAILED.\n      May also include other fields.\n    \"\"\"\n    result = ee.data.getTaskStatus(task_id)[0]\n\n    if result['state'] == 'UNKNOWN':\n        result['state'] = ee.batch.Task.State.UNSUBMITTED\n\n    return result\n\n\ndef is_task_active(task_id):\n    \"\"\"Returns whether the task is still running.\"\"\"\n    return task_status(task_id)['state'] in (ee.batch.Task.State.READY,\n                                             ee.batch.Task.State.RUNNING,\n                                             ee.batch.Task.State.CANCEL_REQUESTED)\n\n\ndef wait_for_task_to_finish(task, end_msg=\"\"):\n    while task.active():\n        print('Polling for task (id: {}).'.format(task.id))\n        time.sleep(5)\n\n    status = task.status()\n    print(status)\n    if (status['state'] != 'FAILED'):\n        if end_msg:\n            print(end_msg)\n        return True, status\n    else:\n        return False, status\n\n\ndef wait_for_task_id_to_finish(task_id, end_msg=\"\"):\n    while is_task_active(task_id):\n        print('Polling for task (id: {}).'.format(task_id))\n        time.sleep(5)\n\n    status = task_status(task_id)\n    print(status)\n    if (status['state'] != 'FAILED'):\n        if end_msg:\n            print(end_msg)\n        return True, status\n    else:\n        print(f\"Completed with status '{status}'\")\n        return False, status\n\n\ndef run_export_func_sync(export_func, table_output, output_filename, output_bucket):\n    task = export_func(table_output, output_filename, output_bucket)\n    task.start()\n    wait_for_task_to_finish(\n        task, f'Task {task.id} is done, saved to {output_filename}')\n\n\ndef generate_export_image_to_storage_task(img_output, output_filename, output_bucket):\n    image, region = img_output\n    # Export a GeoTIF file to Cloud Storage.\n\n    return ee.batch.Export.image.toCloudStorage(**{\n        'image': image,\n        'region':region,\n        'crs': 'EPSG:4326',\n        'scale': 10,\n        'maxPixels':1e13,\n        'bucket': output_bucket,\n        'fileNamePrefix': output_filename,\n    })\n\ndef generate_export_table_to_storage_task(table_output, output_filename, output_bucket):\n    # Export a CSV file to Cloud Storage.\n    return ee.batch.Export.table.toCloudStorage(\n        collection=table_output,\n        bucket=output_bucket,\n        fileNamePrefix=output_filename,\n    )\n\n\n\n\n5. Reading Cloud Images\n\nAccess Locally\nCopy images locally before processing (in case of a need for faster code access to the image, i.e. multiple reads of the whole image for analysis / usage in an ML model).\n\n\n\nwith rasterio.open('data/RGB.byte.tif') as dataset:\n    print(dataset.profile)\n\n\nAccess Remotely\nAccess images remotely via cloud (in case of less frequent read access / need for infrastructure flexibility, i.e a conversion pipeline or displaying the map - assuming data is saved in a “streamable” format - AKA Cloud Optimized GeoTiffs).\n Image Source - planet.com\nwith rasterio.open(\"https://oin-hotosm.s3.amazonaws.com/56f9b5a963ebf4bc00074e70/0/56f9c2d42b67227a79b4faec.tif\") as src:\n    show(src)\n\n\nGDAL and Virtual File Systems\nThis flexibility in access is possible by using the Geospatial Data Abstraction Library - the de-facto standard for reading / writing raster data, and it’s support for virtual file systems. It also supports reading archived data.\n\n\n\n\n\nAdditional read methods\nSome more read / write methods are demonstrated in this gist.\n\n\n\nimage.png\n\n\n\n\n\nEnding B: Visualize Rasters\n\nServe COGs using GeoServer\nhttp://geoserver.org/ https://github.com/geoserver/geoserver\nGeoServer is an open source server for sharing geospatial data. Designed for interoperability, it publishes data from any major spatial data source using open standards, including COGs:\nhttps://docs.geoserver.org/latest/en/user/community/cog/cog.html?highlight=geotiff\n\n\nServe COGs using MapTiler\nhttps://www.maptiler.com/server/\nhttps://documentation.maptiler.com/hc/en-us/articles/4404732284305-Imagery-Hosting- GeoTIFF- MapTiler Server is software for self-hosting data produced by the MapTiler platform. Among other\nthings, it can convert Shapefiles or raster images for super fast map loading, and serve pre- generated map tiles in GeoPackage or MBTiles format.\n\n\nCustom Built Servers\nLibraries that use GDAL as a low level layer for reading GeoTIFFs support streaming raster data from bucket storage and other remote sources, including various libraries used to create tile servers from GeoTIFFs.\nMany of the supported libraries are listed on the COG website: https://www.cogeo.org.\nFor a easy to use Python based server, I recommend Terracotta.\n\n\n\n5. Using Rasterio for Raster Analysis\n\nimport rasterio\n# Open a geospatial dataset\ndataset = rasterio.open(\"https://oin-hotosm.s3.amazonaws.com/56f9b5a963ebf4bc00074e70/0/56f9c2d42b67227a79b4faec.tif\")\nprint(dataset)\n\n<open DatasetReader name='https://oin-hotosm.s3.amazonaws.com/56f9b5a963ebf4bc00074e70/0/56f9c2d42b67227a79b4faec.tif' mode='r'>\n\n\n\n# what is the name of this image\nimg_name = dataset.name\nprint('Image filename: {n}\\n'.format(n=img_name))\n\n# How many bands does this image have?\nnum_bands = dataset.count\nprint('Number of bands in image: {n}\\n'.format(n=num_bands))\n\n# How many rows and columns?\nrows, cols = dataset.shape\nprint('Image size is: {r} rows x {c} columns\\n'.format(r=rows, c=cols))\n\nImage filename: https://oin-hotosm.s3.amazonaws.com/56f9b5a963ebf4bc00074e70/0/56f9c2d42b67227a79b4faec.tif\n\nNumber of bands in image: 3\n\nImage size is: 25201 rows x 20179 columns\n\n\n\n\n# Does the raster have a description or metadata?\ndesc = dataset.descriptions\nmetadata = dataset.meta\n\nprint('Raster description: {desc}\\n'.format(desc=desc))\n\n# What driver was used to open the raster?\ndriver = dataset.driver\nprint('Raster driver: {d}\\n'.format(d=driver))\n\n# What is the raster's projection?\nproj = dataset.crs\nprint('Image projection:')\nprint(proj)\n\nRaster description: (None, None, None)\n\nRaster driver: GTiff\n\nImage projection:\nEPSG:32759 \n\n\n\n\n# What is the raster's \"geo-transform\"\ngt = dataset.transform\n\nprint('Image geo-transform:\\n{gt}\\n'.format(gt=gt))\n\nprint('All raster metadata:')\nprint(metadata)\n\nImage geo-transform:\n| 0.04, 0.00, 229579.08|\n| 0.00,-0.04, 8057866.86|\n| 0.00, 0.00, 1.00|\n\nAll raster metadata:\n{'driver': 'GTiff', 'dtype': 'uint8', 'nodata': None, 'width': 20179, 'height': 25201, 'count': 3, 'crs': CRS.from_epsg(32759), 'transform': Affine(0.0362, 0.0, 229579.08329,\n       0.0, -0.0362, 8057866.857120001)}\n\n\nThe rasterio Dataset object we created contains a lot of useful information but it is not directly used to read in the raster image. Instead we will need to access the raster’s bands using the read() method:\n\n# A window is a view onto a rectangular subset of a raster dataset and is described in rasterio by\n# column and row offsets and width and height in pixels. These may be ints or floats.\nfrom rasterio.windows import Window\nrow_start = 0\nrow_stop = 1000\ncol_start = 0\ncol_stop = 1000\nwindow = Window.from_slices((row_start, row_stop), (col_start, col_stop))\n\n# Open the second band in our image\ndata = dataset.read(window=window)\ndata.shape # check out the dimensions of the image\n\n(3, 1000, 1000)\n\n\n\nimport numpy as np\n\n# What are the image's datatypes?\ndatatype = dataset.dtypes\nprint('Band datatypes: {dt}'.format(dt=datatype))\n\n# How about some image statistics?\nimage_mean = np.mean(data)\nimage_min = np.amin(data)\nimage_max = np.amax(data)\nimage_stddev = np.std(data)\nprint('Image range: {minimum} - {maximum}'.format(maximum=image_max,\n                                                 minimum=image_min))\nprint('Image mean, stddev: {m}, {s}\\n'.format(m=image_mean, s=image_stddev))\n\nBand datatypes: ('uint8', 'uint8', 'uint8')\nImage range: 0 - 0\nImage mean, stddev: 0.0, 0.0\n\n\n\n\n\nEnding C: Use Raster Data as Input for Further Computation\nAs you can see, reading rasterio data generates numpy arrays that can be processed in any Data Analysis / Machine Learning ecosystem. There is no “right” way to proceed - so either use libraries you have experience, or start from tutorials, such as:\n\nhttps://www.drivendata.co/blog/cloud-cover-benchmark/\nhttps://colab.research.google.com/github/ishgirwan/omdena_hdi/blob/master/training_model.ipynb#scrollTo=gsgOsjXXwWhp"
  },
  {
    "objectID": "gee-to-rasterio.html#resources-tutorials",
    "href": "gee-to-rasterio.html#resources-tutorials",
    "title": "Google Earth Engine to Rasterio",
    "section": "Resources / Tutorials",
    "text": "Resources / Tutorials\n\nGeeMap Library Documentation\nEe-mont User Guide\nEnd-to-end GEE\nOpenGeo Tutorial\nPublic GEE Repositories"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "An interactive presentation / tutorial on geospatial analysis using Google Earth Engine and Rasterio"
  },
  {
    "objectID": "index.html#setting-up-the-notebooks-environment",
    "href": "index.html#setting-up-the-notebooks-environment",
    "title": "About",
    "section": "Setting up the notebook’s environment",
    "text": "Setting up the notebook’s environment\n\nPython dependencies\nI recommend running in a virtual environment / colab.\n\npip install jupyter_contrib_nbextensions\npip install RISE\npip install geemap\npip install eemont\npip install autopep8\npip install rasterio\npip install numpy\npip install blockdiagMagic\n\n\nimport time\n\nfrom IPython.display import IFrame\n\n\n\nHow to view this notebook as a slide\nRerun jupyter notebook (via the web UI) after installing RISE, then you should see the following button, which opens the slideshow: \n\nPreview\n\n\n\nMore about slideshows in Jupyter\nHow to generate a slideshow from a notebook\nAdding Rise to be able to run slideshows from Jupyter\n\n\n\nIn Code Diagrams\nDiagrams should appear when Jupyter is run after the blockdiagMagic is installed.\n\nMore about the library\nGithub repo\n\n\nExample\n\n#Reset to png output with: %setdiagpng\n\n\nA -> B -> C;\nB -> D;"
  }
]